{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c813f362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ilomn\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "from scipy import ndimage\n",
    "from skimage.measure import regionprops\n",
    "from skimage import io\n",
    "from skimage.filters import threshold_otsu   # For finding the threshold for grayscale to binary conversion\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2694dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_image_paths_train = \"../Dataset/train/real\"\n",
    "genuine_image_paths_test = \"../Dataset/test/real\"\n",
    "\n",
    "forged_image_paths_train = \"../Dataset/train/forged\"\n",
    "forged_image_paths_test = \"../Dataset/test/forged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cba45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgbgrey(img):\n",
    "    # Converts rgb to grayscale\n",
    "    greyimg = np.zeros((img.shape[0], img.shape[1]))\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[row])):\n",
    "            greyimg[row][col] = np.average(img[row][col])\n",
    "    return greyimg\n",
    "\n",
    "def greybin(img):\n",
    "    # Converts grayscale to binary\n",
    "    blur_radius = 0.8\n",
    "    img = ndimage.gaussian_filter(img, blur_radius)  # to remove small components or noise\n",
    "#     img = ndimage.binary_erosion(img).astype(img.dtype)\n",
    "    thres = threshold_otsu(img)\n",
    "    binimg = img > thres\n",
    "    binimg = np.logical_not(binimg)\n",
    "    return binimg\n",
    "\n",
    "def preproc(path, img=None, display=True):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    if display:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    grey = rgbgrey(img) #rgb to grey\n",
    "    if display:\n",
    "        plt.imshow(grey, cmap = matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    binimg = greybin(grey) #grey to binary\n",
    "    if display:\n",
    "        plt.imshow(binimg, cmap = matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    r, c = np.where(binimg==1)\n",
    "    # Now we will make a bounding box with the boundary as the position of pixels on extreme.\n",
    "    # Thus we will get a cropped image with only the signature part.\n",
    "    signimg = binimg[r.min(): r.max(), c.min(): c.max()]\n",
    "    if display:\n",
    "        plt.imshow(signimg, cmap = matplotlib.cm.Greys_r)\n",
    "        plt.show()\n",
    "    return signimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673d9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ratio(img):\n",
    "    a = 0\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[0])):\n",
    "            if img[row][col]==True:\n",
    "                a = a+1\n",
    "    total = img.shape[0] * img.shape[1]\n",
    "    return a/total\n",
    "\n",
    "def Centroid(img):\n",
    "    numOfWhites = 0\n",
    "    a = np.array([0,0])\n",
    "    for row in range(len(img)):\n",
    "        for col in range(len(img[0])):\n",
    "            if img[row][col]==True:\n",
    "                b = np.array([row,col])\n",
    "                a = np.add(a,b)\n",
    "                numOfWhites += 1\n",
    "    rowcols = np.array([img.shape[0], img.shape[1]])\n",
    "    centroid = a/numOfWhites\n",
    "    centroid = centroid/rowcols\n",
    "    return centroid[0], centroid[1]   # 1st row is the x-component, 2nd row is the y-component\n",
    "\n",
    "def EccentricitySolidity(img):\n",
    "    r = regionprops(img.astype(\"int8\"))\n",
    "    return r[0].eccentricity, r[0].solidity\n",
    "\n",
    "def SkewKurtosis(img):\n",
    "    h,w = img.shape\n",
    "    x = range(w)  # columns value\n",
    "    y = range(h)  # rows value\n",
    "    #calculate projections along the x and y axes\n",
    "    xp = np.sum(img,axis=0)\n",
    "    yp = np.sum(img,axis=1)\n",
    "    #centroid\n",
    "    cx = np.sum(x*xp)/np.sum(xp)\n",
    "    cy = np.sum(y*yp)/np.sum(yp)\n",
    "    #standard deviation\n",
    "    x2 = (x-cx)**2\n",
    "    y2 = (y-cy)**2\n",
    "    sx = np.sqrt(np.sum(x2*xp)/np.sum(img))\n",
    "    sy = np.sqrt(np.sum(y2*yp)/np.sum(img))\n",
    "    \n",
    "    #skewness\n",
    "    x3 = (x-cx)**3\n",
    "    y3 = (y-cy)**3\n",
    "    skewx = np.sum(xp*x3)/(np.sum(img) * sx**3)\n",
    "    skewy = np.sum(yp*y3)/(np.sum(img) * sy**3)\n",
    "\n",
    "    #Kurtosis\n",
    "    x4 = (x-cx)**4\n",
    "    y4 = (y-cy)**4\n",
    "    # 3 is subtracted to calculate relative to the normal distribution\n",
    "    kurtx = np.sum(xp*x4)/(np.sum(img) * sx**4) - 3\n",
    "    kurty = np.sum(yp*y4)/(np.sum(img) * sy**4) - 3\n",
    "\n",
    "    return (skewx , skewy), (kurtx, kurty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21a5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(path, img=None, display=False):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    img = preproc(path, display=display)\n",
    "    ratio = Ratio(img)\n",
    "    centroid = Centroid(img)\n",
    "    eccentricity, solidity = EccentricitySolidity(img)\n",
    "    skewness, kurtosis = SkewKurtosis(img)\n",
    "    retVal = (ratio, centroid, eccentricity, solidity, skewness, kurtosis)\n",
    "    return retVal\n",
    "\n",
    "def getCSVFeatures(path, img=None, display=False):\n",
    "    if img is None:\n",
    "        img = mpimg.imread(path)\n",
    "    temp = getFeatures(path, display=display)\n",
    "    features = (temp[0], temp[1][0], temp[1][1], temp[2], temp[3], temp[4][0], temp[4][1], temp[5][0], temp[5][1])  \n",
    "    '''\n",
    "    2D MATRIX, DATA FORMAT FOR CNN INPUT. Each row vector displays each feature variable. \n",
    "    For example:\n",
    "    \n",
    "            col 1:         col 2:\n",
    "    row 0: [ratio               ]\n",
    "    row 1: [cent_y        cent_x]\n",
    "    row 2: [eccentricity        ]\n",
    "    row 3: [solidity            ]\n",
    "    row 4: [skew_y        skew_x]\n",
    "    row 5: [kurt_x        kurt_y]\n",
    "    ''' \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05c8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCSV(setType):\n",
    "    # if the specified folder does not exist, create it to store the incoming CSV\n",
    "    if not(os.path.exists('../Dataset/Features')):          \n",
    "        os.mkdir('../Dataset/Features')   \n",
    "        print('New folder \"Features\" created')\n",
    "    if not(os.path.exists('../Dataset/Features/Training')):\n",
    "        os.mkdir('../Dataset/Features/Training')\n",
    "        print('New folder \"Features/Training\" created')\n",
    "    if not(os.path.exists('../Dataset/Features/Testing')):\n",
    "        os.mkdir('../Dataset/Features/Testing')\n",
    "        print('New folder \"Features/Testing\" created')\n",
    "   \n",
    "    # Check what type\n",
    "    if(setType != 'train' and setType != 'test'):\n",
    "        print('Invalid type!')\n",
    "        return\n",
    "    \n",
    "    if(setType == 'train'):\n",
    "        gpath = genuine_image_paths_train\n",
    "        fpath = forged_image_paths_train\n",
    "        csvpath = '../Dataset/Features/Training/training_'\n",
    "        setRange = range(1, 70)\n",
    "    else:\n",
    "        gpath = genuine_image_paths_test\n",
    "        fpath = forged_image_paths_test\n",
    "        csvpath = '../Dataset/Features/Testing/testing_'\n",
    "        setRange = range(49, 70)\n",
    "    \n",
    "\n",
    "    # Extracting features based on type defined: 'train' ranges from 1-69, 'test' ranges from 49-69\n",
    "    for person in tqdm(setRange, desc='Extracting Features...'):\n",
    "        per = ('00'+str(person))[-3:]\n",
    "               \n",
    "        # In case folder is not iterative (i.e, 005 does not exist)\n",
    "        if not(os.path.exists(os.path.join(gpath, per))):\n",
    "            continue\n",
    "    \n",
    "        # In case folder is not iterative (i.e, 005 does not exist)\n",
    "        if not(os.path.exists(os.path.join(fpath, per+'_forg'))):\n",
    "            continue\n",
    "    \n",
    "        # every signature image will generate a unique CSV feature row specific to it, hence the concatenation\n",
    "        with open(csvpath+per+'.csv', 'w') as handle:\n",
    "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
    "\n",
    "            # Genuine set\n",
    "            i = 1\n",
    "            fileNum = ('0' + str(i))[-2:]  # this takes care of single vs double digits\n",
    "            filePath = os.path.join(gpath, per,  per + '_' + fileNum + '.png')\n",
    "            while(os.path.exists(filePath)):\n",
    "                source = filePath   # source image taken iteratively based on their own naming convention  \n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',1\\n')  # 1 is set for genuine signatures \n",
    "                i += 1\n",
    "                fileNum = ('0' + str(i))[-2:]  # this takes care of single vs double digits\n",
    "                filePath = os.path.join(gpath, per,  per + '_' + fileNum + '.png')\n",
    "                \n",
    "            # Forged set\n",
    "            i = 1\n",
    "            fileNum = ('0' + str(i))[-2:]  # this takes care of single vs double digits\n",
    "            filePath = os.path.join(fpath, per+'_forg',  per + '_forg_' + fileNum + '.png')\n",
    "            while(os.path.exists(filePath)):\n",
    "                source = filePath   # source image taken iteratively based on their own naming convention  \n",
    "                features = getCSVFeatures(path=source)\n",
    "                handle.write(','.join(map(str, features))+',0\\n')  # 0 is set for genuine signatures \n",
    "                i += 1\n",
    "                fileNum = ('0' + str(i))[-2:]  # this takes care of single vs double digits\n",
    "                filePath = os.path.join(fpath, per+'_forg',  per + '_forg_' + fileNum + '.png')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b893c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(path):    # path is the image specified\n",
    "    feature = getCSVFeatures(path)   # extract features from a specific image into a tuple, suitable for storing into CSV \n",
    "    if not(os.path.exists('../Dataset/TestFeatures')):\n",
    "        os.mkdir('../Dataset/TestFeatures')\n",
    "    with open('../Dataset/TestFeatures/testcsv.csv', 'w') as handle:\n",
    "        # create a new CSV to store the numerical features of the signature image, similar to makeCSV\n",
    "        handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
    "        handle.write(','.join(map(str, feature))+'\\n')\n",
    "\n",
    "def readCSV(train_path, test_path, type2=False):\n",
    "    # Reading train data\n",
    "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
    "    train_input = np.array(df.values)\n",
    "    train_input = train_input.astype(np.float32, copy=False)  # Converting input to float_32\n",
    "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
    "    temp = [elem[0] for elem in df.values]\n",
    "    correct = np.array(temp)\n",
    "    corr_train = keras.utils.to_categorical(correct,2)      # Converting to one hot\n",
    "    \n",
    "    # Reading test data\n",
    "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
    "    test_input = np.array(df.values)\n",
    "    test_input = test_input.astype(np.float32, copy=False)\n",
    "    \n",
    "    if not(type2):\n",
    "        df = pd.read_csv(test_path, usecols=(n_input,))\n",
    "        temp = [elem[0] for elem in df.values]\n",
    "        correct = np.array(temp)\n",
    "        corr_test = keras.utils.to_categorical(correct,2)      # Converting to one hot\n",
    "        return train_input, corr_train, test_input, corr_test\n",
    "    else:\n",
    "        return train_input, corr_train, test_input\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    layer_1 = tf.tanh((tf.matmul(x, weights['h1']) + biases['b1']))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    out_layer = tf.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "def evaluate(train_path, test_path, type2=False):   \n",
    "    if not(type2):\n",
    "        train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path)\n",
    "    else:\n",
    "        train_input, corr_train, test_input = readCSV(train_path, test_path, type2)\n",
    "    ans = 'Random'\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
    "            if cost < 0.0001:\n",
    "                break\n",
    "#              # Display logs per epoch step\n",
    "#             if epoch % 999 == 0:\n",
    "#                 print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(cost))\n",
    "#                 print(\"Optimization Finished!\")\n",
    "#                 print()        \n",
    "        \n",
    "        # Finding accuracies\n",
    "        accuracy1 =  accuracy.eval({X: train_input, Y: corr_train})\n",
    "        print(\"Accuracy for train: {0:.2f} %\".format(accuracy1*100))\n",
    "        if type2 is False:\n",
    "            accuracy2 =  accuracy.eval({X: test_input, Y: corr_test})\n",
    "            print(\"Accuracy for test: {0:.2f} %\".format(round(accuracy2*100)))\n",
    "            return accuracy1, accuracy2\n",
    "        else:\n",
    "            prediction = pred.eval({X: test_input})\n",
    "            if prediction[0][1]>prediction[0][0]:\n",
    "                print('Genuine Image: {0:.1f} %'.format(prediction[0][1]*100))\n",
    "                return True\n",
    "            else:\n",
    "                print('Forged Image: {0:.1f} %'.format(prediction[0][0]*100))\n",
    "                return False\n",
    "\n",
    "\n",
    "def trainAndTest(rate=0.001, epochs=1700, neurons=7, display=False, firstPerson = 49, lastPerson = 69):    \n",
    "    start = time()\n",
    "\n",
    "    # Parameters\n",
    "    global training_rate, training_epochs, n_hidden_1\n",
    "    learning_rate = rate\n",
    "    training_epochs = epochs\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = neurons # 1st layer number of neurons\n",
    "    n_hidden_2 = 7 # 2nd layer number of neurons\n",
    "    n_hidden_3 = 30 # 3rd layer\n",
    "\n",
    "    train_avg, test_avg = 0, 0\n",
    "    n = lastPerson - firstPerson + 1\n",
    "    for i in range(firstPerson, lastPerson+1):\n",
    "        id = ('00'+str(i))[-3:]\n",
    "        if display:\n",
    "            print()\n",
    "            print(\"Running for Person id\",id)\n",
    "        train_score, test_score = evaluate(train_path.replace(('0'+str(firstPerson)),id), test_path.replace(('0'+str(firstPerson)),id))\n",
    "        train_avg += train_score\n",
    "        test_avg += test_score\n",
    "    if display:\n",
    "#         print(\"Number of neurons in Hidden layer-\", n_hidden_1)\n",
    "        print()\n",
    "        print(\"Training average: {0:.2f}%\".format((train_avg/n)*100))\n",
    "        print(\"Testing average:{0:.2f}%\".format((test_avg/n)*100))\n",
    "        print(\"Time taken:{0:.2f}s\".format(time()-start))\n",
    "        print()\n",
    "    return train_avg/n, test_avg/n, (time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e8fae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running for Person id 049\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 75.00 %\n",
      "\n",
      "Running for Person id 050\n",
      "Accuracy for train: 90.00 %\n",
      "Accuracy for test: 75.00 %\n",
      "\n",
      "Running for Person id 051\n",
      "Accuracy for train: 93.75 %\n",
      "Accuracy for test: 75.00 %\n",
      "\n",
      "Running for Person id 052\n",
      "Accuracy for train: 86.36 %\n",
      "Accuracy for test: 83.00 %\n",
      "\n",
      "Running for Person id 053\n",
      "Accuracy for train: 73.91 %\n",
      "Accuracy for test: 40.00 %\n",
      "\n",
      "Running for Person id 054\n",
      "Accuracy for train: 95.83 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 055\n",
      "Accuracy for train: 80.00 %\n",
      "Accuracy for test: 50.00 %\n",
      "\n",
      "Running for Person id 056\n",
      "Accuracy for train: 82.35 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 057\n",
      "Accuracy for train: 90.00 %\n",
      "Accuracy for test: 50.00 %\n",
      "\n",
      "Running for Person id 058\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 059\n",
      "Accuracy for train: 94.12 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 060\n",
      "Accuracy for train: 75.00 %\n",
      "Accuracy for test: 50.00 %\n",
      "\n",
      "Running for Person id 061\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 062\n",
      "Accuracy for train: 80.00 %\n",
      "Accuracy for test: 75.00 %\n",
      "\n",
      "Running for Person id 063\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 75.00 %\n",
      "\n",
      "Running for Person id 064\n",
      "Accuracy for train: 88.24 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 065\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 066\n",
      "Accuracy for train: 91.30 %\n",
      "Accuracy for test: 80.00 %\n",
      "\n",
      "Running for Person id 067\n",
      "Accuracy for train: 82.35 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 068\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Running for Person id 069\n",
      "Accuracy for train: 100.00 %\n",
      "Accuracy for test: 100.00 %\n",
      "\n",
      "Training average: 90.63%\n",
      "Testing average:82.30%\n",
      "Time taken:10.15s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9062967555863517, 0.8230158729212624, 10.153358459472656)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input = 9\n",
    "train_person_id = \"049\"  # eg: 049\n",
    "# key in path of image that you want to check (eg: ../Dataset/test/forged/049_forg/049_forg_01.png):\n",
    "test_image_path = \"../Dataset/test/forged/049_forg/049_forg_02.png\"\n",
    "train_path = '../Dataset/Features/Training/training_049.csv' # specify path of train CSV for that requested person\n",
    "testing(test_image_path)   # make a new seperate CSV to store the features of the specific test image \n",
    "test_path = '../Dataset/Features/Testing/testing_049.csv'  # specify the path of newly created test CSV\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 7 # 1st layer number of neurons\n",
    "n_hidden_2 = 10 # 2nd layer number of neurons\n",
    "n_hidden_3 = 30 # 3rd layer number of neurons\n",
    "n_classes = 2 # no. of classes (genuine or forged)\n",
    "\n",
    "# tf Graph input\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], seed=1)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], seed=2))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1], seed=3)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], seed=4))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.math.squared_difference(logits, Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, var_list=None)\n",
    "\n",
    "# For accuracies\n",
    "pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# evaluate(train_path, test_path, type2=True)\n",
    "trainAndTest(display=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
